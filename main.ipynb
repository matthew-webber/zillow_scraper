{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:152: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:469: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df2, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:470: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df3, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:471: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df4, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:472: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df5, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:473: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df6, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:474: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df7, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:475: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df8, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:476: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df9, ignore_index = True)\n",
      "/var/folders/gh/mq6z90wx7hq2q8rnws8nr2_40000gn/T/ipykernel_70730/3295700196.py:488: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['prices'] = df['prices'].str.replace(r'\\D', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prices</th>\n",
       "      <th>address</th>\n",
       "      <th>bed_bath_sqft</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>729000.0</td>\n",
       "      <td>7905 Bestride Bnd Austin TX 78744</td>\n",
       "      <td>4|3|2446&lt;abbr class=\"list-card-label\"&gt; &lt;!-- -...</td>\n",
       "      <td>7905 Bestride Bnd Austin TX 78744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564999.0</td>\n",
       "      <td>1145 Strickland Dr Austin TX 78748</td>\n",
       "      <td>3|3|1723&lt;abbr class=\"list-card-label\"&gt; &lt;!-- -...</td>\n",
       "      <td>1145 Strickland Dr Austin TX 78748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5999900.0</td>\n",
       "      <td>4811 Palisade Dr Austin TX 78731</td>\n",
       "      <td>5|4|4130&lt;abbr class=\"list-card-label\"&gt; &lt;!-- -...</td>\n",
       "      <td>4811 Palisade Dr Austin TX 78731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>349500.0</td>\n",
       "      <td>14304 Vandever St Austin TX 78725</td>\n",
       "      <td>3|2|1347&lt;abbr class=\"list-card-label\"&gt; &lt;!-- -...</td>\n",
       "      <td>14304 Vandever St Austin TX 78725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>399000.0</td>\n",
       "      <td>7207 Ritchie Dr Austin TX 78724</td>\n",
       "      <td>3|2|1605&lt;abbr class=\"list-card-label\"&gt; &lt;!-- -...</td>\n",
       "      <td>7207 Ritchie Dr Austin TX 78724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prices                               address                                      bed_bath_sqft                               links\n",
       "0   729000.0    7905 Bestride Bnd Austin TX 78744    4|3|2446<abbr class=\"list-card-label\"> <!-- -...   7905 Bestride Bnd Austin TX 78744\n",
       "1   564999.0   1145 Strickland Dr Austin TX 78748    3|3|1723<abbr class=\"list-card-label\"> <!-- -...  1145 Strickland Dr Austin TX 78748\n",
       "2  5999900.0     4811 Palisade Dr Austin TX 78731    5|4|4130<abbr class=\"list-card-label\"> <!-- -...    4811 Palisade Dr Austin TX 78731\n",
       "3   349500.0    14304 Vandever St Austin TX 78725    3|2|1347<abbr class=\"list-card-label\"> <!-- -...   14304 Vandever St Austin TX 78725\n",
       "4   399000.0      7207 Ritchie Dr Austin TX 78724    3|2|1605<abbr class=\"list-card-label\"> <!-- -...     7207 Ritchie Dr Austin TX 78724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column datatypes are:\n",
      "prices           float64\n",
      "address           object\n",
      "bed_bath_sqft     object\n",
      "links             object\n",
      "dtype: object\n",
      "The dataframe shape is: (90, 4)\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '7905 Bestride Bnd Austin TX 78744': No scheme supplied. Perhaps you meant http://7905 Bestride Bnd Austin TX 78744?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMissingSchema\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 538>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    537\u001B[0m zillow_zestimate \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m link \u001B[38;5;129;01min\u001B[39;00m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinks\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m--> 539\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43ms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlink\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq_headers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m     soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(r\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    541\u001B[0m     home_value \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mselect_one(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh4:contains(\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHome value\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/deletemetest/venv/lib/python3.9/site-packages/requests/sessions.py:600\u001B[0m, in \u001B[0;36mSession.get\u001B[0;34m(self, url, **kwargs)\u001B[0m\n\u001B[1;32m    592\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001B[39;00m\n\u001B[1;32m    593\u001B[0m \n\u001B[1;32m    594\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m    595\u001B[0m \u001B[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001B[39;00m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    599\u001B[0m kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 600\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/deletemetest/venv/lib/python3.9/site-packages/requests/sessions.py:573\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;66;03m# Create the Request.\u001B[39;00m\n\u001B[1;32m    561\u001B[0m req \u001B[38;5;241m=\u001B[39m Request(\n\u001B[1;32m    562\u001B[0m     method\u001B[38;5;241m=\u001B[39mmethod\u001B[38;5;241m.\u001B[39mupper(),\n\u001B[1;32m    563\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    571\u001B[0m     hooks\u001B[38;5;241m=\u001B[39mhooks,\n\u001B[1;32m    572\u001B[0m )\n\u001B[0;32m--> 573\u001B[0m prep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    575\u001B[0m proxies \u001B[38;5;241m=\u001B[39m proxies \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m    577\u001B[0m settings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_environment_settings(\n\u001B[1;32m    578\u001B[0m     prep\u001B[38;5;241m.\u001B[39murl, proxies, stream, verify, cert\n\u001B[1;32m    579\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/deletemetest/venv/lib/python3.9/site-packages/requests/sessions.py:484\u001B[0m, in \u001B[0;36mSession.prepare_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    481\u001B[0m     auth \u001B[38;5;241m=\u001B[39m get_netrc_auth(request\u001B[38;5;241m.\u001B[39murl)\n\u001B[1;32m    483\u001B[0m p \u001B[38;5;241m=\u001B[39m PreparedRequest()\n\u001B[0;32m--> 484\u001B[0m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupper\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    486\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfiles\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdict_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCaseInsensitiveDict\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_setting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcookies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerged_cookies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhooks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmerge_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhooks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "File \u001B[0;32m~/PycharmProjects/deletemetest/venv/lib/python3.9/site-packages/requests/models.py:368\u001B[0m, in \u001B[0;36mPreparedRequest.prepare\u001B[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001B[39;00m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_method(method)\n\u001B[0;32m--> 368\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_headers(headers)\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_cookies(cookies)\n",
      "File \u001B[0;32m~/PycharmProjects/deletemetest/venv/lib/python3.9/site-packages/requests/models.py:439\u001B[0m, in \u001B[0;36mPreparedRequest.prepare_url\u001B[0;34m(self, url, params)\u001B[0m\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;241m*\u001B[39me\u001B[38;5;241m.\u001B[39margs)\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m scheme:\n\u001B[0;32m--> 439\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MissingSchema(\n\u001B[1;32m    440\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid URL \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m: No scheme supplied. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    441\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPerhaps you meant http://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    442\u001B[0m     )\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m host:\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidURL(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid URL \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m: No host supplied\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mMissingSchema\u001B[0m: Invalid URL '7905 Bestride Bnd Austin TX 78744': No scheme supplied. Perhaps you meant http://7905 Bestride Bnd Austin TX 78744?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# \"# from IPython.display import display\\n\",\n",
    "# \"\\n\",\n",
    "# \"# create list of python .gitignore files\\n\",\n",
    "# \"        \\n\",\n",
    "# \"\\n\",\n",
    "# \"#set some display settings for notebooks\\n\",\n",
    "# \"pd.set_option('display.max_rows', 500)\\n\",\n",
    "# \"pd.set_option('display.max_columns', 500)\\n\",\n",
    "# \"pd.set_option('display.width', 1000)\\n\",\n",
    "# \"\\n\",\n",
    "\n",
    "\n",
    "#add headers in case you use chromedriver (captchas are no fun); namely used for chromedriver\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "#create url variables for each zillow page\n",
    "with requests.Session() as s:\n",
    "    city = 'austin/' #*****change this city to what you want!!!!*****\n",
    "\n",
    "    url = f'https://www.zillow.com/homes/for_sale/{city}'\n",
    "    url2 = f'https://www.zillow.com/homes/for_sale/{city}/2_p/'\n",
    "    url3 = f'https://www.zillow.com/homes/for_sale/{city}/3_p/'\n",
    "    url4 = f'https://www.zillow.com/homes/for_sale/{city}/4_p/'\n",
    "    url5 = f'https://www.zillow.com/homes/for_sale/{city}/5_p/'\n",
    "    url6 = f'https://www.zillow.com/homes/for_sale/{city}/6_p/'\n",
    "    url7 = f'https://www.zillow.com/homes/for_sale/{city}/7_p/'\n",
    "    url8 = f'https://www.zillow.com/homes/for_sale/{city}/8_p/'\n",
    "    url9 = f'https://www.zillow.com/homes/for_sale/{city}/9_p/'\n",
    "    url10 = f'https://www.zillow.com/homes/for_sale/{city}/10_p/'\n",
    "\n",
    "    r = s.get(url, headers=req_headers)\n",
    "    r2 = s.get(url2, headers=req_headers)\n",
    "    r3 = s.get(url3, headers=req_headers)\n",
    "    r4 = s.get(url4, headers=req_headers)\n",
    "    r5 = s.get(url5, headers=req_headers)\n",
    "    r6 = s.get(url6, headers=req_headers)\n",
    "    r7 = s.get(url7, headers=req_headers)\n",
    "    r8 = s.get(url8, headers=req_headers)\n",
    "    r9 = s.get(url9, headers=req_headers)\n",
    "    r10 = s.get(url10, headers=req_headers)\n",
    "\n",
    "    url_links = [url, url2, url3, url4, url5, url6, url7, url8, url9, url10]\n",
    "\n",
    "#add contents of urls to soup variable from each url\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "soup1 = BeautifulSoup(r2.content, 'html.parser')\n",
    "soup2 = BeautifulSoup(r3.content, 'html.parser')\n",
    "soup3 = BeautifulSoup(r4.content, 'html.parser')\n",
    "soup4 = BeautifulSoup(r5.content, 'html.parser')\n",
    "soup5 = BeautifulSoup(r6.content, 'html.parser')\n",
    "soup6 = BeautifulSoup(r7.content, 'html.parser')\n",
    "soup7 = BeautifulSoup(r8.content, 'html.parser')\n",
    "soup8 = BeautifulSoup(r9.content, 'html.parser')\n",
    "soup9 = BeautifulSoup(r10.content, 'html.parser')\n",
    "\n",
    "# page_links = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]\n",
    "\n",
    "#create the first two dataframes\n",
    "# df = pd.DataFrame()\n",
    "# df1 = pd.DataFrame()\n",
    "\n",
    "#all for loops are pulling the specified variable using beautiful soup and inserting into said variable\n",
    "for i in soup:\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    # MLW broken\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    # df['prices'] = price\n",
    "    # df['address'] = address\n",
    "    # df['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "#     print(href)\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "#         addresses = 'zillow doesnt like this piece of code!'\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "#     print(addresses)\n",
    "#     addresses = addresses.get_text()\n",
    "#     addresses = href.find_all(href=True)\n",
    "#     addresses = href.find('address class')\n",
    "#     addresses.extract()\n",
    "#     print(addresses)\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "df['links'] = urls\n",
    "df['links'] = df['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df['links'] = df['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df['links'] = df['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup1:\n",
    "    address1 = soup1.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup1.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup1.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup1.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup1.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup1.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup1.find_all (class_= 'list-card-link')\n",
    "#     zestimate1 = soup1.find_all (class_= 'Text-c11n-8-38-0__aiai24-0 jtMauM')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df1['prices'] = price1\n",
    "    df1['address'] = address1\n",
    "    df1['beds'] = beds\n",
    "#     df1['zestimate'] = zestimate1\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup1.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df1['links'] = urls\n",
    "df1['links'] = df1['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df1['links'] = df1['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df1['links'] = df1['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append first two dataframes\n",
    "df = df.append(df1, ignore_index = True)\n",
    "\n",
    "#create empty dataframes\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "df4 = pd.DataFrame()\n",
    "df5 = pd.DataFrame()\n",
    "df6 = pd.DataFrame()\n",
    "df7 = pd.DataFrame()\n",
    "df8 = pd.DataFrame()\n",
    "df9 = pd.DataFrame()\n",
    "\n",
    "for i in soup2:\n",
    "    soup = soup2\n",
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df2['prices'] = price\n",
    "    df2['address'] = address\n",
    "    df2['beds'] = beds\n",
    "\n",
    "time.sleep(3.2)\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup2.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df2['links'] = urls\n",
    "df2['links'] = df2['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df2['links'] = df2['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df2['links'] = df2['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "#append n df\n",
    "# df = df.append(df2, ignore_index = True)\n",
    "# display(df)\n",
    "# time.sleep(3.1)\n",
    "\n",
    "for i in soup3:\n",
    "    soup = soup3\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df3['prices'] = price1\n",
    "    df3['address'] = address1\n",
    "    df3['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup3.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df3['links'] = urls\n",
    "df3['links'] = df3['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df3['links'] = df3['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df3['links'] = df3['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup4:\n",
    "    soup = soup4\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df4['prices'] = price1\n",
    "    df4['address'] = address1\n",
    "    df4['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup4.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df4['links'] = urls\n",
    "df4['links'] = df4['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df4['links'] = df4['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df4['links'] = df4['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup5:\n",
    "    soup = soup5\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df5['prices'] = price1\n",
    "    df5['address'] = address1\n",
    "    df5['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup5.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df5['links'] = urls\n",
    "df5['links'] = df5['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df5['links'] = df5['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df5['links'] = df5['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup6:\n",
    "    soup = soup6\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df6['prices'] = price1\n",
    "    df6['address'] = address1\n",
    "    df6['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup6.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#import urls into a links column\n",
    "df6['links'] = urls\n",
    "df6['links'] = df6['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df6['links'] = df6['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df6['links'] = df6['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup7:\n",
    "    soup = soup7\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df7['prices'] = price1\n",
    "    df7['address'] = address1\n",
    "    df7['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup7.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df7['links'] = urls\n",
    "df7['links'] = df7['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df7['links'] = df7['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df7['links'] = df7['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup8:\n",
    "    soup = soup8\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df8['prices'] = price1\n",
    "    df8['address'] = address1\n",
    "    df8['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup8.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df8['links'] = urls\n",
    "df8['links'] = df8['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df8['links'] = df8['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df8['links'] = df8['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "for i in soup9:\n",
    "    soup = soup9\n",
    "    address1 = soup.find_all (class_= 'list-card-addr')\n",
    "    price1 = list(soup.find_all (class_='list-card-price'))\n",
    "    beds1 = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details1 = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type1 = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated1 = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage1 = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link1 = soup.find_all (class_= 'list-card-link')\n",
    "\n",
    "    #create dataframe columns out of variables\n",
    "    df9['prices'] = price1\n",
    "    df9['address'] = address1\n",
    "    df9['beds'] = beds\n",
    "\n",
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in soup9.find_all(\"article\"):\n",
    "    href = link.find('a',class_=\"list-card-link\")\n",
    "    if href == None:\n",
    "        addresses = None\n",
    "    else:\n",
    "        addresses = href.find('address',class_=\"list-card-addr\").get_text()\n",
    "    urls.append(addresses)\n",
    "    urls = [x for x in urls if x is not None]\n",
    "\n",
    "#import urls into a links column\n",
    "df9['links'] = urls\n",
    "df9['links'] = df9['links'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "# df9['links'] = df9['links'].replace('<a class=\"list-card-link\" href=\"', ' ', regex=True)\n",
    "# df9['links'] = df9['links'].replace('\" tabindex=\"0\"></a>', ' ', regex=True)\n",
    "\n",
    "df = df.append(df2, ignore_index = True)\n",
    "df = df.append(df3, ignore_index = True)\n",
    "df = df.append(df4, ignore_index = True)\n",
    "df = df.append(df5, ignore_index = True)\n",
    "df = df.append(df6, ignore_index = True)\n",
    "df = df.append(df7, ignore_index = True)\n",
    "df = df.append(df8, ignore_index = True)\n",
    "df = df.append(df9, ignore_index = True)\n",
    "\n",
    "#convert columns to str\n",
    "df['prices'] = df['prices'].astype('str')\n",
    "df['address'] = df['address'].astype('str')\n",
    "df['beds'] = df['beds'].astype('str')\n",
    "\n",
    "#remove html tags\n",
    "df['prices'] = df['prices'].replace('<div class=\"list-card-price\">', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('<address class=\"list-card-addr\">', ' ', regex=True)\n",
    "df['prices'] = df['prices'].replace('</div>', ' ', regex=True)\n",
    "df['address'] = df['address'].replace('</address>', ' ', regex=True)\n",
    "df['prices'] = df['prices'].str.replace(r'\\D', '')\n",
    "\n",
    "#remove html tags from beds column\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li class=\"\">', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bds</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->bd</abbr></li><li>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace(' abbr></li><li class=\"\">', '|', regex=True)\n",
    "df['beds'] = df['beds'].replace('Studio</li><li>', '0 ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->ba</', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace(' abbr></li><li class=\"\">', '|', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- House for sale</li></ul>', ' ', regex=True)\n",
    "df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->sqft</abbr></li><li class=\"list-card-statusText\">- Condo for sale</li></ul>', ' ', regex=True)\n",
    "\n",
    "df.columns = ['prices','address','bed_bath_sqft','links']\n",
    "#split beds column into beds, bath and sq_feet\n",
    "# df[['beds','baths','sq_feet']] = df.beds.str.split(expand=True)\n",
    "# df[['baths']] = df.beds.str.split('|',expand=True)\n",
    "# df[['sq_feet']] = df.baths.str.split('|',expand=True)\n",
    "\n",
    "#remove commas from sq_feet and convert to float\n",
    "df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "#drop nulls\n",
    "df = df[(df['prices'] != '') & (df['prices']!= ' ')]\n",
    "\n",
    "#convert column to float\n",
    "df['prices'] = df['prices'].astype('float')\n",
    "# d['sq_feet'] = df['sq_feet'].astype('float')\n",
    "\n",
    "#remove spaces from link column\n",
    "# df['links'] = df.links.str.replace(' ','')\n",
    "\n",
    "#display\n",
    "display(df.head())\n",
    "\n",
    "print('The column datatypes are:')\n",
    "print(df.dtypes)\n",
    "print('The dataframe shape is:', df.shape)\n",
    "\n",
    "#rearrange the columns\n",
    "# df = df[['prices', 'address', 'links', 'beds', 'baths', 'sq_feet']]\n",
    "\n",
    "# df\n",
    "\n",
    "#calculate the zestimate and insert into a dataframe\n",
    "zillow_zestimate = []\n",
    "for link in df['links']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    home_value = soup.select_one('h4:contains(\"Home value\")')\n",
    "    if not home_value:\n",
    "        home_value = soup.select_one('.zestimate').text.split()[-1]\n",
    "    else:\n",
    "        home_value = home_value.find_next('p').get_text(strip=True)\n",
    "    zillow_zestimate.append(home_value)\n",
    "\n",
    "cols=['zestimate']\n",
    "zestimate_result = pd.DataFrame(zillow_zestimate, columns=cols)\n",
    "# zestimate_result\n",
    "\n",
    "#convert zestimate column to float, and remove , and $\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('$','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace('/mo','')\n",
    "zestimate_result['zestimate'] = zestimate_result['zestimate'].str.replace(',','')\n",
    "\n",
    "#covert rows with non zestimate to 0\n",
    "def non_zestimate(zestimate_result):\n",
    "    if len(zestimate_result['zestimate']) > 20:\n",
    "        return '0'\n",
    "    elif len(zestimate_result['zestimate']) < 5:\n",
    "        return '0'\n",
    "    else:\n",
    "        return zestimate_result['zestimate']\n",
    "\n",
    "zestimate_result['zestimate'] = zestimate_result.apply(non_zestimate,axis=1)\n",
    "\n",
    "# zestimate_result\n",
    "\n",
    "#concat zestimate dataframe and original df\n",
    "df = pd.concat([df, zestimate_result], axis=1)\n",
    "df['zestimate'] = df['zestimate'].astype('float')\n",
    "\n",
    "#create best deal column and sort by best_deal\n",
    "df ['best_deal'] = df['prices'] - df['zestimate']\n",
    "df = df.sort_values(by='best_deal')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<address class=\"list-card-addr\">1808 Ford St UNIT A, Austin, TX 78704</address>,\n",
       " <address class=\"list-card-addr\">504 Canion St, Austin, TX 78752</address>,\n",
       " <address class=\"list-card-addr\">(undisclosed Address), Austin, TX 78747</address>,\n",
       " <address class=\"list-card-addr\">10518 Signal Hill Rd, Austin, TX 78737</address>,\n",
       " <address class=\"list-card-addr\">48 East Ave #3108, Austin, TX 78701</address>,\n",
       " <address class=\"list-card-addr\">84 East Ave #2902, Austin, TX 78701</address>,\n",
       " <address class=\"list-card-addr\">4115 Eskew Dr, Austin, TX 78749</address>,\n",
       " <address class=\"list-card-addr\">1023 E 45th St #2, Austin, TX 78751</address>,\n",
       " <address class=\"list-card-addr\">5916 Corrine Ln, Austin, TX 78747</address>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    address = soup.find_all (class_= 'list-card-addr')\n",
    "    price = list(soup.find_all (class_='list-card-price'))\n",
    "    beds = list(soup.find_all(\"ul\", class_=\"list-card-details\"))\n",
    "    details = soup.find_all ('div', {'class': 'list-card-details'})\n",
    "    home_type = soup.find_all ('div', {'class': 'list-card-footer'})\n",
    "    last_updated = soup.find_all ('div', {'class': 'list-card-top'})\n",
    "    brokerage = list(soup.find_all(class_= 'list-card-brokerage list-card-img-overlay',text=True))\n",
    "    link = soup.find_all (class_= 'list-card-link')\n",
    "    \n",
    "    address"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}